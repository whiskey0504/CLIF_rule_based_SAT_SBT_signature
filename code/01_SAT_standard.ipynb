{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import duckdb\n",
    "import pyCLIF as pc\n",
    "import pySBT as t1code\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from tableone import TableOne, load_dataset\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "con = pc.load_config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = pd.read_csv('../output/intermediate/study_cohort.csv')\n",
    "t1_cohort = cohort.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the full directory path\n",
    "directory_path = os.path.join(\"../output/final/\", pc.helper[\"site_name\"], \"SAT_standard\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "    print(f\"Directory '{directory_path}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{directory_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort['sat_delivery_pass_fail'] = cohort['sat_delivery_pass_fail'].map({0:1,1:1})\n",
    "cohort['sat_screen_pass_fail'] = cohort['sat_screen_pass_fail'].map({0:1,1:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'event_time' is in datetime format\n",
    "cohort[\"event_time\"] = pd.to_datetime(cohort[\"event_time\"])\n",
    "cohort[\"admission_dttm\"] = pd.to_datetime(cohort[\"admission_dttm\"], utc=True)\n",
    "cohort[\"discharge_dttm\"] = pd.to_datetime(cohort[\"discharge_dttm\"], utc=True)\n",
    "\n",
    "# Ensure the data is sorted by 'hosp_id_day_key' and 'event_time'\n",
    "cohort = cohort.sort_values(by=[\"hospitalization_id\", \"event_time\"]).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "cohort[\"device_category_ffill\"] = cohort.groupby(\"hospitalization_id\")[\n",
    "    \"device_category\"\n",
    "].ffill()\n",
    "cohort[\"location_category_ffill\"] = cohort.groupby(\"hospitalization_id\")[\n",
    "    \"location_category\"\n",
    "].ffill()\n",
    "\n",
    "active_sedation_n_col = [\n",
    "    \"fentanyl\",\n",
    "    \"propofol\",\n",
    "    \"lorazepam\",\n",
    "    \"midazolam\",\n",
    "    \"hydromorphone\",\n",
    "    \"morphine\",\n",
    "]\n",
    "\n",
    "for col in active_sedation_n_col:\n",
    "    if col not in cohort.columns:\n",
    "        cohort[col] = np.nan\n",
    "        print(\n",
    "            f\"Column '{col}' is missing. Please check your CLIF Meds table — it might be missing, or it's okay if your site doesn't use it.\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Fill forward the meds by hospitalization columns by 'hosp_id'\n",
    "cohort[\n",
    "    [\"fentanyl\", \"propofol\", \"lorazepam\", \"midazolam\", \"hydromorphone\", \"morphine\"]\n",
    "] = cohort.groupby(\"hospitalization_id\")[\n",
    "    [\"fentanyl\", \"propofol\", \"lorazepam\", \"midazolam\", \"hydromorphone\", \"morphine\"]\n",
    "].ffill()\n",
    "\n",
    "# Ensure the min value is greater than 0\n",
    "cohort[\"min_sedation_dose\"] = cohort[\n",
    "    [\"fentanyl\", \"propofol\", \"lorazepam\", \"midazolam\", \"hydromorphone\", \"morphine\"]\n",
    "].min(axis=1, skipna=True)\n",
    "cohort[\"min_sedation_dose_2\"] = (\n",
    "    cohort[\n",
    "        [\"fentanyl\", \"propofol\", \"lorazepam\", \"midazolam\", \"hydromorphone\", \"morphine\"]\n",
    "    ]\n",
    "    .where(\n",
    "        cohort[\n",
    "            [\n",
    "                \"fentanyl\",\n",
    "                \"propofol\",\n",
    "                \"lorazepam\",\n",
    "                \"midazolam\",\n",
    "                \"hydromorphone\",\n",
    "                \"morphine\",\n",
    "            ]\n",
    "        ]\n",
    "        > 0\n",
    "    )\n",
    "    .min(axis=1, skipna=True)\n",
    ")\n",
    "cohort[\"min_sedation_dose_non_ops\"] = cohort[\n",
    "    [\"propofol\", \"lorazepam\", \"midazolam\"]\n",
    "].min(axis=1, skipna=True)\n",
    "cohort[\"min_sedation_dose_non_ops\"] = cohort[\"min_sedation_dose_non_ops\"].fillna(0)\n",
    "\n",
    "# Fill forward the paralytic by hospitalization columns by 'hosp_id'\n",
    "cohort[[\"cisatracurium\", \"vecuronium\", \"rocuronium\"]] = cohort.groupby(\n",
    "    \"hospitalization_id\"\n",
    ")[[\"cisatracurium\", \"vecuronium\", \"rocuronium\"]].ffill()\n",
    "# paralytic max to remove from consideration\n",
    "cohort[\"max_paralytics\"] = (\n",
    "    cohort[[\"cisatracurium\", \"vecuronium\", \"rocuronium\"]]\n",
    "    .max(axis=1, skipna=True)\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "\n",
    "# Fill Rass\n",
    "cohort[[\"rass\"]] = cohort.groupby(\"hospitalization_id\")[[\"rass\"]].ffill()\n",
    "\n",
    "# Ensure the data is sorted again by 'hosp_id_day_key' and 'event_time'\n",
    "cohort = cohort.sort_values(by=[\"hospitalization_id\", \"event_time\"]).reset_index(\n",
    "    drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify eligible days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cohort(df):\n",
    "    df = df.sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)\n",
    "    df['device_category_ffill'] = df.groupby('hospitalization_id')['device_category'].ffill()\n",
    "    df['location_category_ffill'] = df.groupby('hospitalization_id')['location_category'].ffill()\n",
    "    # Ensure 'event_time' is datetime\n",
    "    df['event_time'] = pd.to_datetime(df['event_time'])\n",
    "   \n",
    "    df['all_conditions_check'] = (\n",
    "            (df['device_category_ffill'].str.lower() == 'imv') &\n",
    "            (df['min_sedation_dose_2'] > 0) &\n",
    "            (df['location_category_ffill'].str.lower() == 'icu') &\n",
    "            (df['max_paralytics'] <= 0)\n",
    "        ).astype(int)\n",
    "\n",
    "    # Initialize result list\n",
    "    result = []\n",
    "\n",
    "    vented_day = df[(df['device_category'] == 'imv')]['hosp_id_day_key'].unique()\n",
    "    # Group by 'hospitalization_id' and 'date'\n",
    "    grouped_hosp = df[df['hosp_id_day_key'].isin(vented_day)].groupby(['hospitalization_id', df['event_time'].dt.normalize()])\n",
    "    \n",
    "    # Use tqdm for the outer loop to show progress\n",
    "    for (hosp_id, date), group in tqdm(grouped_hosp, desc='Evaluating hospitalizations by day for SAT eligibility'):\n",
    "        group = group.sort_values('event_time')\n",
    "\n",
    "        # Get the entire hospitalization data for the current hospitalization_id\n",
    "        temp_df = df[df['hospitalization_id'] == hosp_id].sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)\n",
    "\n",
    "        # Define start and end times for the current day\n",
    "        # Start time is 10 PM of the previous day\n",
    "        start_time = date - pd.Timedelta(days=1) + pd.Timedelta(hours=22)\n",
    "        # End time is 6 AM of the current day\n",
    "        end_time = date + pd.Timedelta(hours=6)\n",
    "\n",
    "        # Filter data in this time window for the entire hospitalization\n",
    "        mask_time = (temp_df['event_time'] >= start_time) & (temp_df['event_time'] <= end_time)\n",
    "        df_time_window = temp_df[mask_time].copy()\n",
    "\n",
    "        if df_time_window.empty:\n",
    "            continue\n",
    "\n",
    "        # Use the existing 'device_category_ffill' and 'location_category_ffill' columns\n",
    "        df_time_window['all_conditions_met'] = (df_time_window['all_conditions_check']>0\n",
    "        )\n",
    "\n",
    "        # If no times where all conditions are met, skip\n",
    "        if not df_time_window['all_conditions_met'].any():\n",
    "            continue\n",
    "\n",
    "        # Ensure data is sorted by 'event_time'\n",
    "        df_time_window = df_time_window.sort_values('event_time').reset_index(drop=True)\n",
    "\n",
    "        # Create a group identifier for continuous periods where conditions are met\n",
    "        df_time_window['condition_met_group'] = (df_time_window['all_conditions_met'] != df_time_window['all_conditions_met'].shift()).cumsum()\n",
    "\n",
    "        # Filter rows where all conditions are met\n",
    "        df_conditions = df_time_window[df_time_window['all_conditions_met']].copy()\n",
    "        if df_conditions.empty:\n",
    "            continue\n",
    "\n",
    "        # Group by 'condition_met_group' to identify continuous periods\n",
    "        grouped_conditions = df_conditions.groupby('condition_met_group')\n",
    "\n",
    "        found_four_hours = False\n",
    "        for group_id, group_df in grouped_conditions:\n",
    "            group_df = group_df.reset_index(drop=True)\n",
    "\n",
    "            # Calculate the duration of each continuous period where all conditions are met\n",
    "            group_df['duration'] = group_df['event_time'].diff().fillna(pd.Timedelta(seconds=0))\n",
    "            group_df['cumulative_duration'] = group_df['duration'].cumsum()\n",
    "            total_duration = group_df['cumulative_duration'].iloc[-1]\n",
    "\n",
    "            if total_duration >= pd.Timedelta(hours=4):\n",
    "                # Calculate the exact event_time when cumulative duration reaches four hours\n",
    "                cumulative_duration = pd.Timedelta(seconds=0)\n",
    "                for idx in range(len(group_df)):\n",
    "                    cumulative_duration += group_df['duration'].iloc[idx]\n",
    "                    if cumulative_duration >= pd.Timedelta(hours=4):\n",
    "                        event_time_at_4_hours = group_df['event_time'].iloc[idx]\n",
    "                        break\n",
    "\n",
    "                # Append to result\n",
    "                result.append({\n",
    "                    'hospitalization_id': hosp_id,\n",
    "                    'current_day_key': date,\n",
    "                    'event_time_at_4_hours': event_time_at_4_hours\n",
    "                })\n",
    "                found_four_hours = True\n",
    "                # Since we found a period of at least 4 hours continuous conditions met, we can proceed to the next day\n",
    "                break  # Exit the loop over condition_met_group\n",
    "        if found_four_hours:\n",
    "            continue  # Proceed to the next day\n",
    "\n",
    "    # Convert result to DataFrame for better representation\n",
    "    result_df = pd.DataFrame(result)\n",
    "    return result_df\n",
    "\n",
    "result_df = process_cohort(cohort)\n",
    "print('Encounter days with at least 4 hours of conditions met from 10 PM to 6 AM:', len(result_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the result back into the cohort DataFrame\n",
    "cohort = cohort.merge(result_df[['hospitalization_id', 'current_day_key', 'event_time_at_4_hours']], \n",
    "                      how='left', \n",
    "                      left_on=['hospitalization_id', cohort['event_time'].dt.normalize()], \n",
    "                      right_on=['hospitalization_id', 'current_day_key'])\n",
    "\n",
    "# Initialize 'eligible_event' column with NaN and used for validation of exact time the event of 4 hr completed\n",
    "cohort['eligible_event'] = np.nan\n",
    "has_event_time = cohort['event_time_at_4_hours'].notna()\n",
    "for (hosp_id, date), group in cohort[has_event_time].groupby(['hospitalization_id', cohort['event_time'].dt.normalize()]):\n",
    "    event_time_at_4_hours = group['event_time_at_4_hours'].iloc[0]\n",
    "    subset = cohort[(cohort['hospitalization_id'] == hosp_id) & (cohort['event_time'] >= event_time_at_4_hours)]\n",
    "    if not subset.empty:\n",
    "        idx = subset['event_time'].idxmin()\n",
    "        cohort.loc[idx, 'eligible_event'] = 1\n",
    "    else:\n",
    "        subset = cohort[cohort['hospitalization_id'] == hosp_id]\n",
    "        idx = subset['event_time'].idxmax()\n",
    "        cohort.loc[idx, 'eligible_event'] = 1\n",
    "\n",
    "# fix where last row should not be eligible\n",
    "cohort = cohort.sort_values(['hospitalization_id', 'event_time']).reset_index(drop=True)\n",
    "for hosp_id, group in cohort.groupby('hospitalization_id'):\n",
    "    last_idx = group.index[-1]\n",
    "    if cohort.loc[last_idx, 'eligible_event'] == 1:\n",
    "        cohort.loc[last_idx, 'eligible_event'] = np.nan\n",
    "\n",
    "\n",
    "# Flag all that date rows where eligible_event = 1\n",
    "filtered_cohort = cohort[cohort['eligible_event'] == 1][['hosp_id_day_key', 'eligible_event']]\n",
    "merged_cohort = cohort.merge(filtered_cohort, on='hosp_id_day_key', how='left', suffixes=('', '_filtered'))\n",
    "merged_cohort['on_vent_and_sedation'] = merged_cohort['eligible_event_filtered'].fillna(0).astype(int)\n",
    "merged_cohort = merged_cohort.drop(columns=['eligible_event_filtered'])\n",
    "\n",
    "del filtered_cohort,result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_cohort['eligible_event'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_cohort[merged_cohort['on_vent_and_sedation']==1]['hosp_id_day_key'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = merged_cohort[merged_cohort['on_vent_and_sedation']==1].sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)  \n",
    "\n",
    "df['rank_sedation'] = np.nan\n",
    "for hosp_id_day_key, hosp_data in tqdm(df[df['on_vent_and_sedation'] == 1].groupby('hosp_id_day_key'), desc='Detecting weaning or tapering behavior for sedation meds'):\n",
    "    zero_mask = hosp_data['min_sedation_dose'] == 0\n",
    "    ranks = zero_mask.cumsum() * zero_mask\n",
    "    df.loc[hosp_data.index, 'rank_sedation'] = ranks.replace(0, np.nan)\n",
    "\n",
    "\n",
    "df['rank_sedation_non_ops'] = np.nan\n",
    "for hosp_id_day_key, hosp_data in tqdm(df[df['on_vent_and_sedation'] == 1].groupby('hosp_id_day_key'), desc='Detecting weaning or tapering behavior for non opioids sedation meds'):\n",
    "    zero_mask = hosp_data['min_sedation_dose_non_ops'] == 0\n",
    "    ranks = zero_mask.cumsum() * zero_mask\n",
    "    df.loc[hosp_data.index, 'rank_sedation_non_ops'] = ranks.replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick rass check\n",
    "df['rass'] = df['rass'].astype(float)\n",
    "df['rass'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAT EHR Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "med_columns      = ['fentanyl', 'propofol', 'lorazepam', 'midazolam', 'hydromorphone', 'morphine']\n",
    "med_columns2     = ['propofol', 'lorazepam', 'midazolam']\n",
    "flags = [\n",
    "    'SAT_EHR_delivery',\n",
    "    'SAT_modified_delivery',\n",
    "    'SAT_rass_nonneg_30',\n",
    "    'SAT_med_halved_rass_pos',\n",
    "    'SAT_no_meds_rass_pos_45',  # new: no meds & rass >=0 in 45 min\n",
    "    'SAT_rass_first_neg_30_last45_nonneg'\n",
    "]\n",
    "for f in flags:\n",
    "    df[f] = np.nan\n",
    "\n",
    "vent_df = df[df['on_vent_and_sedation'] == 1]\n",
    "delta30 = pd.Timedelta(minutes=30)\n",
    "delta45 = pd.Timedelta(minutes=45)\n",
    "\n",
    "# Main loop\n",
    "for key, group in tqdm(vent_df.groupby('hosp_id_day_key'), desc='Evaluating SAT using RASS, meds, and ventilation criteria. (6 Ways)'):\n",
    "    grp = group.sort_values('event_time')\n",
    "    times = grp['event_time'].values\n",
    "    ranks = grp['rank_sedation'].values\n",
    "    idxs  = grp.index\n",
    "\n",
    "    for idx, current_time, rank in zip(idxs, times, ranks):\n",
    "        if pd.isna(rank):\n",
    "            continue\n",
    "\n",
    "        # Define windows\n",
    "        fw30 = grp[(grp['event_time'] >= current_time) & (grp['event_time'] <= current_time + delta30)]\n",
    "        fw45 = grp[(grp['event_time'] >= current_time) & (grp['event_time'] <= current_time + delta45)]\n",
    "        pr30 = grp[(grp['event_time'] >= current_time - delta30) & (grp['event_time'] <  current_time)]\n",
    "\n",
    "        # ICU+IMV check\n",
    "        imv_ok = (fw30['device_category_ffill'] == 'imv').all()\n",
    "        icu_ok = (fw30['location_category_ffill'] == 'icu').all()\n",
    "\n",
    "        if imv_ok and icu_ok:\n",
    "            # No meds at all in next 30 min\n",
    "            meds_ok  = (fw30[med_columns].isna() | (fw30[med_columns] == 0)).all().all()\n",
    "            if meds_ok:\n",
    "                df.at[idx, 'SAT_EHR_delivery'] = 1\n",
    "\n",
    "            # No subset meds\n",
    "            meds2_ok = (fw30[med_columns2].isna() | (fw30[med_columns2] == 0)).all().all()\n",
    "            if meds2_ok:\n",
    "                df.at[idx, 'SAT_modified_delivery'] = 1\n",
    "\n",
    "            # RASS >= 0 in next 30 min\n",
    "            rass30 = fw30['rass'].dropna()\n",
    "            if not rass30.empty and (rass30 >= 0).all():\n",
    "                df.at[idx, 'SAT_rass_nonneg_30'] = 1\n",
    "\n",
    "            # New: meds halved & last RASS45 >= 0\n",
    "            if not pr30.empty and not fw30.empty:\n",
    "                # Compute 50% thresholds\n",
    "                half_max = pr30[med_columns].max() * 0.5\n",
    "\n",
    "                # Only check non-NaN, non-zero meds in forward window\n",
    "                halved_ok = True\n",
    "                for med in med_columns:\n",
    "                    vals = fw30[med].dropna()\n",
    "                    vals = vals[vals != 0]  # ignore zero doses\n",
    "                    if not vals.empty and not (vals <= half_max[med]).all():\n",
    "                        halved_ok = False\n",
    "                        break\n",
    "\n",
    "                # Last non-null RASS in 45 window\n",
    "                rass45 = fw45['rass'].dropna()\n",
    "                rass45_ok = not rass45.empty and rass45.iloc[-1] >= 0\n",
    "\n",
    "                if halved_ok and rass45_ok:\n",
    "                    df.at[idx, 'SAT_med_halved_rass_pos'] = 1\n",
    "\n",
    "            # New: no meds & last RASS45 >= 0\n",
    "            if meds_ok:\n",
    "                rass45 = fw45['rass'].dropna()\n",
    "                rass45_ok = not rass45.empty and rass45.iloc[-1] >= 0\n",
    "                if rass45_ok:\n",
    "                    df.at[idx, 'SAT_no_meds_rass_pos_45'] = 1\n",
    "            \n",
    "                # --- new flag: first RASS <0 in next 30 & last RASS ≥0 in next 45 ---\n",
    "            rass45 = fw45['rass'].dropna()\n",
    "            if not rass30.empty and not rass45.empty:\n",
    "                first_rass30 = rass30.iloc[0]\n",
    "                last_rass45  = rass45.iloc[-1]\n",
    "                if first_rass30 < 0 and last_rass45 >= 0:\n",
    "                    df.at[idx, 'SAT_rass_first_neg_30_last45_nonneg'] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAtch for non RASS sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making all rass relased flags to zero\n",
    "if cohort[\"rass\"].nunique() <= 1:\n",
    "    for x in [\n",
    "        \"SAT_rass_nonneg_30\",\n",
    "        \"SAT_med_halved_rass_pos\",\n",
    "        \"SAT_no_meds_rass_pos_45\",  # new: no meds & rass >=0 in 45 min\n",
    "        \"SAT_rass_first_neg_30_last45_nonneg\",\n",
    "    ]:\n",
    "        df[x] = 0\n",
    "        print('Your SITE CLIF dont have RASS!! making all RASS Flags to Zero')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Identify initial sat failure events and delivery events\n",
    "mask_initial = (df['sat_delivery_pass_fail'] == 1) | (df['sat_screen_pass_fail'] == 1)\n",
    "mask_ehr = df['SAT_EHR_delivery'] == 1\n",
    "mask_mod = df['SAT_modified_delivery'] == 1\n",
    "\n",
    "initial_times = df[mask_initial].groupby('hosp_id_day_key')['event_time'].min().rename('initial_time')\n",
    "ehr_times = df[mask_ehr].groupby('hosp_id_day_key')['event_time'].min().rename('ehr_time')\n",
    "mod_times = df[mask_mod].groupby('hosp_id_day_key')['event_time'].min().rename('mod_time')\n",
    "\n",
    "# 2. Merge into a single DataFrame and drop incomplete cases\n",
    "times_df = pd.concat([initial_times, ehr_times, mod_times], axis=1).dropna()\n",
    "\n",
    "# 3. Convert event_time columns to datetime\n",
    "for col in ['initial_time', 'ehr_time', 'mod_time']:\n",
    "    times_df[col] = pd.to_datetime(times_df[col])\n",
    "\n",
    "# 4. Compute deltas in minutes\n",
    "times_df['delta_to_ehr'] = (times_df['ehr_time'] - times_df['initial_time']).dt.total_seconds() / 60\n",
    "times_df['delta_to_mod'] = (times_df['mod_time'] - times_df['initial_time']).dt.total_seconds() / 60\n",
    "\n",
    "# 5. Filter deltas to positive values within 24 hours (0–1440 minutes)\n",
    "times_df = times_df[\n",
    "    (times_df['delta_to_ehr'] >= 0) & (times_df['delta_to_ehr'] <= 1440) &\n",
    "    (times_df['delta_to_mod'] >= 0) & (times_df['delta_to_mod'] <= 1440)\n",
    "]\n",
    "\n",
    "# 6. Bin into hourly intervals\n",
    "bins = list(range(0, 24*60 + 1, 60))  # [0, 60, 120, ..., 1440]\n",
    "labels = [f'{i}-{i+1}hr' for i in range(24)]\n",
    "\n",
    "ehr_binned = pd.cut(times_df['delta_to_ehr'], bins=bins, labels=labels, right=False)\n",
    "mod_binned = pd.cut(times_df['delta_to_mod'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "ehr_counts = ehr_binned.value_counts().sort_index()\n",
    "mod_counts = mod_binned.value_counts().sort_index()\n",
    "\n",
    "# # 7. Plot both lines hour-wise\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(labels, ehr_counts.values, marker='o', label='To SAT_EHR_delivery')\n",
    "# plt.plot(labels, mod_counts.values, marker='s', label='To SAT_modified_delivery')\n",
    "# plt.xlabel('Hours since initial failure event')\n",
    "# plt.ylabel('Count of Hospital-Day Keys')\n",
    "# plt.title('Hourly Distribution of Time to EHR and Modified Deliveries')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "binned_df = pd.DataFrame({\n",
    "    'hour_bin': labels,\n",
    "    'count_to_SAT_EHR_delivery': ehr_counts.values,\n",
    "    'count_to_SAT_modified_delivery': mod_counts.values\n",
    "})\n",
    "binned_df.to_csv(f'{directory_path}/binned_delta_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Icu los calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_los = cohort[['hospitalization_id', 'event_time', 'location_category_ffill']]\n",
    "icu_los = icu_los.sort_values(by=['hospitalization_id', 'event_time']).reset_index(drop=True)\n",
    "\n",
    "icu_los['segment'] = (icu_los['location_category_ffill'] != icu_los['location_category_ffill'].shift()).cumsum()\n",
    "\n",
    "icu_segments = icu_los[icu_los['location_category_ffill'].str.lower() == 'icu'].groupby(\n",
    "    ['hospitalization_id', 'segment']\n",
    ").agg(\n",
    "    location_start=('event_time', 'first'),\n",
    "    location_end=('event_time', 'last')\n",
    ").reset_index()\n",
    "\n",
    "icu_segments['los_days'] = (icu_segments['location_end'] - icu_segments['location_start']).dt.total_seconds() / (24 * 3600)\n",
    "icu_los_per_encounter = icu_segments[['hospitalization_id', 'los_days']]\n",
    "\n",
    "total_icu_los_per_hosp = icu_los_per_encounter.groupby('hospitalization_id', as_index=False).agg(\n",
    "    ICU_LOS=('los_days', 'sum')\n",
    ")\n",
    "total_icu_los_per_hosp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### last dishcharge hosptial_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hosp = cohort[['hospitalization_id', 'event_time', 'hospital_id']]\n",
    "\n",
    "last_hosp = last_hosp.sort_values(by=['hospitalization_id','event_time'], ascending=False).groupby(\n",
    "    ['hospitalization_id'], as_index=False\n",
    ").agg(({'hospital_id': 'first'})).reset_index(drop=True)\n",
    "last_hosp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table one df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = df[['patient_id', 'hospitalization_id', 'admission_dttm', 'discharge_dttm',\n",
    "       'age_at_admission', 'discharge_category', 'sex_category',\n",
    "       'race_category', 'ethnicity_category','hosp_id_day_key']].drop_duplicates()\n",
    "main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = pd.merge(main, total_icu_los_per_hosp, on='hospitalization_id', how='left')\n",
    "main = pd.merge(main, last_hosp, on='hospitalization_id', how='left')\n",
    "main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to group by\n",
    "group_cols = [\n",
    " 'hosp_id_day_key'\n",
    "]\n",
    "\n",
    "max_cols = ['sat_screen_pass_fail','sat_delivery_pass_fail','SAT_EHR_delivery', 'SAT_modified_delivery', 'eligible_event','SAT_EHR_delivery',\n",
    "    'SAT_modified_delivery',\n",
    "    'SAT_rass_nonneg_30',\n",
    "    'SAT_med_halved_rass_pos',\n",
    "    'SAT_no_meds_rass_pos_45','SAT_rass_first_neg_30_last45_nonneg']\n",
    "agg_dict = {col: 'max' for col in max_cols}\n",
    "\n",
    "df_grouped = df.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "\n",
    "df_grouped = df_grouped.sort_values('hosp_id_day_key').reset_index(drop=True)\n",
    "\n",
    "df_grouped['sat_flowsheet_delivery_flag'] = np.where(\n",
    "    (\n",
    "        (df_grouped['sat_screen_pass_fail'] == 1) |\n",
    "        (df_grouped['sat_delivery_pass_fail'] == 1)\n",
    "    ) &\n",
    "    (df_grouped['eligible_event'] == 1),\n",
    "    1,  # Flag is set to 1 (True) if conditions are met\n",
    "    np.nan   # Flag nan\n",
    ")\n",
    "\n",
    "final_df = main.merge(df_grouped, on='hosp_id_day_key', how='inner')\n",
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [\n",
    "    \"sat_delivery_pass_fail\",\n",
    "    \"sat_screen_pass_fail\",\n",
    "    \"SAT_EHR_delivery\",\n",
    "\n",
    "    \"SAT_modified_delivery\",\n",
    "    \"eligible_event\",\n",
    "\n",
    "    \"sat_flowsheet_delivery_flag\",\n",
    "    \"SAT_rass_first_neg_30_last45_nonneg\",\n",
    "    \"SAT_rass_nonneg_30\",\n",
    "    \"SAT_med_halved_rass_pos\",\n",
    "    \"SAT_no_meds_rass_pos_45\",\n",
    "]:\n",
    "\n",
    "    print(final_df[x].value_counts())\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_df = final_df.copy()\n",
    "for col in ['SAT_EHR_delivery', 'SAT_modified_delivery', 'sat_flowsheet_delivery_flag' , \"SAT_rass_nonneg_30\",\n",
    "    \"SAT_med_halved_rass_pos\",\n",
    "    \"SAT_no_meds_rass_pos_45\",'SAT_rass_first_neg_30_last45_nonneg']:\n",
    "    con_df[col].fillna(0, inplace=True)\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "for col in ['SAT_EHR_delivery', 'SAT_modified_delivery' ,\"SAT_rass_nonneg_30\",\n",
    "    \"SAT_med_halved_rass_pos\",\n",
    "    \"SAT_no_meds_rass_pos_45\",'SAT_rass_first_neg_30_last45_nonneg']:\n",
    "    y_true = con_df['sat_flowsheet_delivery_flag']\n",
    "    y_pred = con_df[col]\n",
    "\n",
    "    if 'rass' in col and cohort[\"rass\"].nunique() <= 1:\n",
    "        continue\n",
    "    \n",
    "    # raw confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    total = cm.sum()\n",
    "\n",
    "    # derived metrics\n",
    "    accuracy    = (tp + tn) / total\n",
    "    precision   = tp / (tp + fp) if (tp + fp) else 0\n",
    "    recall      = tp / (tp + fn) if (tp + fn) else 0\n",
    "    f1          = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) else 0\n",
    "\n",
    "    # normalized matrix for percentages\n",
    "    cm_pct = cm / total * 100  # percent\n",
    "\n",
    "    # plot\n",
    "    labels = [\"No Delivery\", \"Delivery\"]\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, cmap=plt.cm.Blues)\n",
    "\n",
    "    # ticks and labels\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_xlabel(f\"{col} Flag\")\n",
    "    ax.set_ylabel(\"Flowsheet Delivery Flag\")\n",
    "    ax.set_title(f\"Concordance: flowsheet vs {col}\")\n",
    "\n",
    "    # annotate each cell with count + percent\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            count = cm[i, j]\n",
    "            pct   = cm_pct[i, j]\n",
    "            ax.text(\n",
    "                j, i,\n",
    "                f\"{count}\\n({pct:.1f}%)\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > cm.max()/2 else \"black\"\n",
    "            )\n",
    "\n",
    "    # colorbar\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "    # save and close\n",
    "    plot_path = os.path.join(directory_path, f\"confusion_matrix_{col}.png\")\n",
    "    fig.savefig(plot_path, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    # print metrics\n",
    "    print(f\"--- Concordance for {col} ---\")\n",
    "    print(f\"Accuracy    : {accuracy:.3f}\")\n",
    "    print(f\"Precision   : {precision:.3f}\")\n",
    "    print(f\"Recall      : {recall:.3f}\")\n",
    "    print(f\"F1 Score    : {f1:.3f}\")\n",
    "    print(f\"Specificity : {specificity:.3f}\\n\")\n",
    "\n",
    "    metrics_list.append({\n",
    "        \"Column\": col,\n",
    "        \"TP\": tp, \"FP\": fp, \"FN\": fn, \"TN\": tn,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Specificity\": specificity\n",
    "    })\n",
    "\n",
    "# save summary\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "metrics_df.to_csv(os.path.join(directory_path, \"delivery_concordance_summary.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('../output/intermediate/final_df_SAT.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### table one print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['sex_category', 'race_category', 'ethnicity_category','discharge_category']\n",
    "non_categorical_columns = ['age_at_admission',  'ICU_LOS', 'Inpatient_LOS']\n",
    "\n",
    "final_df['admission_dttm'] = pd.to_datetime(final_df['admission_dttm'],utc=True)\n",
    "final_df['discharge_dttm'] = pd.to_datetime(final_df['discharge_dttm'],utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAT FLAG Table 1\n",
    "\n",
    "\n",
    "sat_flow_t1 = final_df[final_df['sat_flowsheet_delivery_flag'] == 1][[ 'hospitalization_id', 'admission_dttm', 'discharge_dttm', 'age_at_admission', 'discharge_category', 'sex_category','race_category', 'ethnicity_category','ICU_LOS']].drop_duplicates()\n",
    "sat_flow_t1['Inpatient_LOS'] = (sat_flow_t1['discharge_dttm'] - sat_flow_t1['admission_dttm']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "if len(sat_flow_t1)>1:\n",
    "    table1 = TableOne(sat_flow_t1, categorical=categorical_columns, nonnormal=non_categorical_columns, columns=categorical_columns+non_categorical_columns )\n",
    "\n",
    "    table1.to_csv(f'{directory_path}/table1_sat_flowhseet_{pc.helper[\"site_name\"]}.csv')\n",
    "    print(table1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAT EHR FLAG Table 1\n",
    "\n",
    "sat_ehr_t1 = final_df[(final_df['SAT_EHR_delivery'] == 1) | (final_df['SAT_modified_delivery'] == 1)][[ 'hospitalization_id', 'admission_dttm', 'discharge_dttm', 'age_at_admission', 'discharge_category', 'sex_category','race_category', 'ethnicity_category','ICU_LOS']].drop_duplicates()\n",
    "sat_ehr_t1['Inpatient_LOS'] = (sat_ehr_t1['discharge_dttm'] - sat_ehr_t1['admission_dttm']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "if len(sat_ehr_t1)>1:\n",
    "    table2 = TableOne(sat_ehr_t1, categorical=categorical_columns, nonnormal=non_categorical_columns, columns=categorical_columns+non_categorical_columns )\n",
    "\n",
    "    table2.to_csv(f'{directory_path}/table1_sat_ehr_{pc.helper[\"site_name\"]}.csv')\n",
    "    print(table2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### all Table 1\n",
    "\n",
    "all_t1 = final_df[[ 'hospitalization_id', 'admission_dttm', 'discharge_dttm', 'age_at_admission', 'discharge_category', 'sex_category','race_category', 'ethnicity_category','ICU_LOS']].drop_duplicates()\n",
    "all_t1['Inpatient_LOS'] = (all_t1['discharge_dttm'] - all_t1['admission_dttm']).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "if len(all_t1)>1:\n",
    "    table3 = TableOne(all_t1, categorical=categorical_columns, nonnormal=non_categorical_columns, columns=categorical_columns+non_categorical_columns )\n",
    "\n",
    "    table3.to_csv(f'{directory_path}/table1_all_t1_{pc.helper[\"site_name\"]}.csv')\n",
    "    print(table3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per hospital stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store each hospital's data\n",
    "data_list = []\n",
    "\n",
    "# Iterate over unique hospital IDs as strings\n",
    "for x in final_df['hospital_id'].astype(str).unique():\n",
    "    # Count of eligible events for this site\n",
    "    eligible_event_count = final_df[\n",
    "        (final_df['eligible_event'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ].shape[0]\n",
    "\n",
    "    # Existing SAT flags counts\n",
    "    sat_flowsheet_delivery_flag_count = final_df[\n",
    "        (final_df['sat_flowsheet_delivery_flag'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ].shape[0]\n",
    "    SAT_modified_delivery_count = final_df[\n",
    "        (final_df['SAT_modified_delivery'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ].shape[0]\n",
    "    SAT_EHR_delivery_count = final_df[\n",
    "        (final_df['SAT_EHR_delivery'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ].shape[0]\n",
    "\n",
    "    # New SAT flags counts\n",
    "    SAT_rass_nonneg_30_count = final_df[\n",
    "        (final_df['SAT_rass_nonneg_30'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ].shape[0]\n",
    "    SAT_med_halved_rass_pos_count = final_df[\n",
    "        (final_df['SAT_med_halved_rass_pos'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ].shape[0]\n",
    "    SAT_no_meds_rass_pos_45_count = final_df[\n",
    "        (final_df['SAT_no_meds_rass_pos_45'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ].shape[0]\n",
    "    # New flag: first negative RASS within 30 min, last 45 min non-negative\n",
    "    SAT_rass_first_neg_30_last45_nonneg_count = final_df[\n",
    "        (final_df['SAT_rass_first_neg_30_last45_nonneg'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ].shape[0]\n",
    "\n",
    "    # Unique patients and hospitalizations for each flag\n",
    "    SAT_EHR_uni_pats = final_df[\n",
    "        (final_df['SAT_EHR_delivery'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ]['patient_id'].nunique()\n",
    "    SAT_EHR_hosp = final_df[\n",
    "        (final_df['SAT_EHR_delivery'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ]['hospitalization_id'].nunique()\n",
    "\n",
    "    SAT_modified_uni_pats = final_df[\n",
    "        (final_df['SAT_modified_delivery'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ]['patient_id'].nunique()\n",
    "    SAT_modified_hosp = final_df[\n",
    "        (final_df['SAT_modified_delivery'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ]['hospitalization_id'].nunique()\n",
    "\n",
    "    SAT_rass_nonneg_30_uni_pats = final_df[\n",
    "        (final_df['SAT_rass_nonneg_30'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ]['patient_id'].nunique()\n",
    "    SAT_rass_nonneg_30_hosp = final_df[\n",
    "        (final_df['SAT_rass_nonneg_30'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ]['hospitalization_id'].nunique()\n",
    "\n",
    "    SAT_med_halved_rass_pos_uni_pats = final_df[\n",
    "        (final_df['SAT_med_halved_rass_pos'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ]['patient_id'].nunique()\n",
    "    SAT_med_halved_rass_pos_hosp = final_df[\n",
    "        (final_df['SAT_med_halved_rass_pos'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ]['hospitalization_id'].nunique()\n",
    "\n",
    "    SAT_no_meds_rass_pos_45_uni_pats = final_df[\n",
    "        (final_df['SAT_no_meds_rass_pos_45'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ]['patient_id'].nunique()\n",
    "    SAT_no_meds_rass_pos_45_hosp = final_df[\n",
    "        (final_df['SAT_no_meds_rass_pos_45'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ]['hospitalization_id'].nunique()\n",
    "\n",
    "    # Unique for the new flag\n",
    "    SAT_rass_first_neg_30_last45_nonneg_uni_pats = final_df[\n",
    "        (final_df['SAT_rass_first_neg_30_last45_nonneg'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ]['patient_id'].nunique()\n",
    "    SAT_rass_first_neg_30_last45_nonneg_hosp = final_df[\n",
    "        (final_df['SAT_rass_first_neg_30_last45_nonneg'] == 1) &\n",
    "        (final_df['hospital_id'].astype(str) == x)\n",
    "    ]['hospitalization_id'].nunique()\n",
    "\n",
    "    # Calculate percentages safely\n",
    "    if eligible_event_count > 0:\n",
    "        percent_sat_flowsheet_delivery_flag = sat_flowsheet_delivery_flag_count / eligible_event_count * 100\n",
    "        percent_SAT_modified_delivery = SAT_modified_delivery_count / eligible_event_count * 100\n",
    "        percent_SAT_EHR_delivery = SAT_EHR_delivery_count / eligible_event_count * 100\n",
    "        percent_SAT_rass_nonneg_30 = SAT_rass_nonneg_30_count / eligible_event_count * 100\n",
    "        percent_SAT_med_halved_rass_pos = SAT_med_halved_rass_pos_count / eligible_event_count * 100\n",
    "        percent_SAT_no_meds_rass_pos_45 = SAT_no_meds_rass_pos_45_count / eligible_event_count * 100\n",
    "        percent_SAT_rass_first_neg_30_last45_nonneg = SAT_rass_first_neg_30_last45_nonneg_count / eligible_event_count * 100\n",
    "    else:\n",
    "        percent_sat_flowsheet_delivery_flag = percent_SAT_modified_delivery = 0\n",
    "        percent_SAT_EHR_delivery = percent_SAT_rass_nonneg_30 = 0\n",
    "        percent_SAT_med_halved_rass_pos = percent_SAT_no_meds_rass_pos_45 = 0\n",
    "        percent_SAT_rass_first_neg_30_last45_nonneg = 0\n",
    "\n",
    "    # Append the metrics for this hospital\n",
    "    data_list.append({\n",
    "        'Site_Name_Hosp': pc.helper[\"site_name\"] + '_' + x,\n",
    "        '%_of_SAT_flowsheet_delivery_flag': percent_sat_flowsheet_delivery_flag,\n",
    "        '%_of_SAT_modified_delivery': percent_SAT_modified_delivery,\n",
    "        '%_of_SAT_EHR_delivery': percent_SAT_EHR_delivery,\n",
    "        '%_of_SAT_rass_nonneg_30': percent_SAT_rass_nonneg_30,\n",
    "        '%_of_SAT_med_halved_rass_pos': percent_SAT_med_halved_rass_pos,\n",
    "        '%_of_SAT_no_meds_rass_pos_45': percent_SAT_no_meds_rass_pos_45,\n",
    "        '%_of_SAT_rass_first_neg_30_last45_nonneg': percent_SAT_rass_first_neg_30_last45_nonneg,\n",
    "        'eligible_event_count': eligible_event_count,\n",
    "        'sat_flowsheet_delivery_flag_count': sat_flowsheet_delivery_flag_count,\n",
    "        'SAT_modified_delivery_count': SAT_modified_delivery_count,\n",
    "        'SAT_EHR_delivery_count': SAT_EHR_delivery_count,\n",
    "        'SAT_rass_nonneg_30_count': SAT_rass_nonneg_30_count,\n",
    "        'SAT_med_halved_rass_pos_count': SAT_med_halved_rass_pos_count,\n",
    "        'SAT_no_meds_rass_pos_45_count': SAT_no_meds_rass_pos_45_count,\n",
    "        'SAT_rass_first_neg_30_last45_nonneg_count': SAT_rass_first_neg_30_last45_nonneg_count,\n",
    "        'SAT_EHR_unique_patients': SAT_EHR_uni_pats,\n",
    "        'SAT_EHR_unique_hospitalizations': SAT_EHR_hosp,\n",
    "        'SAT_modified_unique_patients': SAT_modified_uni_pats,\n",
    "        'SAT_modified_unique_hospitalizations': SAT_modified_hosp,\n",
    "        'SAT_rass_nonneg_30_unique_patients': SAT_rass_nonneg_30_uni_pats,\n",
    "        'SAT_rass_nonneg_30_unique_hospitalizations': SAT_rass_nonneg_30_hosp,\n",
    "        'SAT_med_halved_rass_pos_unique_patients': SAT_med_halved_rass_pos_uni_pats,\n",
    "        'SAT_med_halved_rass_pos_unique_hospitalizations': SAT_med_halved_rass_pos_hosp,\n",
    "        'SAT_no_meds_rass_pos_45_unique_patients': SAT_no_meds_rass_pos_45_uni_pats,\n",
    "        'SAT_no_meds_rass_pos_45_unique_hospitalizations': SAT_no_meds_rass_pos_45_hosp,\n",
    "        'SAT_rass_first_neg_30_last45_nonneg_unique_patients': SAT_rass_first_neg_30_last45_nonneg_uni_pats,\n",
    "        'SAT_rass_first_neg_30_last45_nonneg_unique_hospitalizations': SAT_rass_first_neg_30_last45_nonneg_hosp,\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the list and export\n",
    "final_data_df = pd.DataFrame(data_list)\n",
    "final_data_df.to_csv(f'{directory_path}/sat_stats_{pc.helper[\"site_name\"]}.csv', index=False)\n",
    "# Display the final DataFrame\n",
    "final_data_df.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Table 1 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate functions\n",
    "def documented(series):\n",
    "    return \"Documented\" if series.notna().any() else \"Not Documented\"\n",
    "\n",
    "def age_bucket(mean_age):\n",
    "    if pd.isna(mean_age):\n",
    "        return None\n",
    "    elif mean_age < 40:\n",
    "        return \"18-39\"\n",
    "    elif mean_age < 60:\n",
    "        return \"40-59\"\n",
    "    elif mean_age < 80:\n",
    "        return \"60-79\"\n",
    "    else:\n",
    "        return \"80+\"\n",
    "\n",
    "# Clean 'language_name' to only \"English\", \"Spanish\", or \"Other\"\n",
    "def categorize_language(lang):\n",
    "    if re.search(r'english', str(lang), re.IGNORECASE):\n",
    "        return 'English'\n",
    "    elif re.search(r'spanish', str(lang), re.IGNORECASE):\n",
    "        return 'Spanish'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "t1_col = [\n",
    "    \"patient_id\",\n",
    "    \"hospitalization_id\",\n",
    "    \"hosp_id_day_key\",\n",
    "    \"age_at_admission\",    \"sex_category\",    \"race_category\",    \"ethnicity_category\",    \"language_name\",    \"weight_kg\",\n",
    "    \"height_cm\", \"cisatracurium\",    \"vecuronium\",    \"rocuronium\",    \"dobutamine\",    \"dopamine\",    \"epinephrine\",\n",
    "    \"fentanyl\",    \"hydromorphone\",    \"isoproterenol\",    \"lorazepam\",    \"midazolam\",    \"milrinone\",    \"morphine\",\n",
    "    \"norepinephrine\",    \"phenylephrine\",    \"propofol\",    \"vasopressin\",    \"angiotensin\",     \"rass\", \"gcs_total\"\n",
    "]\n",
    "\n",
    "medication_columns = [\n",
    "    \"rass\", \"gcs_total\", \"cisatracurium\", \"vecuronium\", \"rocuronium\",\n",
    "    \"dobutamine\", \"dopamine\", \"epinephrine\", \"fentanyl\", \"hydromorphone\",\n",
    "    \"isoproterenol\", \"lorazepam\", \"midazolam\", \"milrinone\", \"morphine\",\n",
    "    \"norepinephrine\", \"phenylephrine\", \"propofol\", \"vasopressin\", \"angiotensin\"\n",
    "]\n",
    "\n",
    "demographic_columns = [\"sex_category\", \"race_category\", \"ethnicity_category\", \"language_name\"]\n",
    "\n",
    "continuous_cols = [\n",
    "    \"rass\", \"gcs_total\", \"cisatracurium\", \"vecuronium\", \"rocuronium\",\n",
    "    \"dobutamine\", \"dopamine\", \"epinephrine\", \"fentanyl\", \"hydromorphone\",\n",
    "    \"isoproterenol\", \"lorazepam\", \"midazolam\", \"milrinone\", \"morphine\",\n",
    "    \"norepinephrine\", \"phenylephrine\", \"propofol\", \"vasopressin\",\n",
    "    \"angiotensin\", \"bmi\"\n",
    "]\n",
    "\n",
    "drugs = [\n",
    "    \"cisatracurium\", \"vecuronium\", \"rocuronium\",\n",
    "    \"dobutamine\", \"dopamine\", \"epinephrine\", \"fentanyl\", \"hydromorphone\",\n",
    "    \"isoproterenol\", \"lorazepam\", \"midazolam\", \"milrinone\", \"morphine\",\n",
    "    \"norepinephrine\", \"phenylephrine\", \"propofol\", \"vasopressin\", \"angiotensin\"\n",
    "]\n",
    "\n",
    "# Apply the transformation\n",
    "t1_cohort[drugs] = t1_cohort[drugs].applymap(lambda x: x if x > 0 else np.nan)\n",
    "\n",
    "t1_cohort['bmi'] = t1_cohort['weight_kg'] / ((t1_cohort['height_cm'] / 100) ** 2)\n",
    "\n",
    "# Apply the function to 'language_name'\n",
    "t1_cohort['language_name'] = t1_cohort['language_name'].apply(categorize_language)\n",
    "t1_cohort['rass'] = t1_cohort['rass'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 1 By ID for Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ['hospitalization_id', 'patient_id']:\n",
    "    t1_summary = t1_cohort.groupby(x).agg(\n",
    "        {\n",
    "            \"age_at_admission\": \"mean\",\n",
    "            **{col: documented for col in medication_columns},\n",
    "            **{col: \"first\" for col in demographic_columns}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    t1_summary[\"age_bucket\"] = t1_summary[\"age_at_admission\"].apply(age_bucket)\n",
    "    t1_summary = t1_summary.drop(columns=[\"age_at_admission\"])\n",
    "    t1_summary = t1_summary.reset_index()\n",
    "\n",
    "    summary_df = t1code.manual_categorical_tableone(\n",
    "        t1_summary, \n",
    "        medication_columns + demographic_columns + [\"age_bucket\"]\n",
    "    )\n",
    "\n",
    "    if x == 'hospitalization_id':\n",
    "        summary_df.to_csv(f\"{directory_path}/table1_hospitalization_id_categorical.csv\", index=False)\n",
    "    else:\n",
    "        summary_df.to_csv(f\"{directory_path}/table1_patient_id_categorical.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 1 By ID for Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitalization_summary = None\n",
    "patient_summary = None\n",
    "\n",
    "hosp = (\n",
    "    t1_cohort\n",
    "      .groupby(\"hospitalization_id\")\n",
    "      .agg(\n",
    "        {\n",
    "          **{c: \"median\" for c in continuous_cols}\n",
    "        }\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "patient = (\n",
    "    t1_cohort\n",
    "      .groupby(\"patient_id\")\n",
    "      .agg(\n",
    "        {\n",
    "          **{c: \"median\" for c in continuous_cols}\n",
    "        }\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Build for hospitalization level and patient level\n",
    "hospitalization_summary = t1code.manual_tableone(hosp, continuous_cols)\n",
    "patient_summary = t1code.manual_tableone(patient, continuous_cols)\n",
    "\n",
    "hospitalization_summary.to_csv(f\"{directory_path}/table1_hospitalization_id_continuous.csv\", index=False)\n",
    "patient_summary.to_csv(f\"{directory_path}/table1_patient_id_continuous.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 1 By Days for Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tqdm(\n",
    "    [\n",
    "        \"eligible_event\",\n",
    "        \"SAT_EHR_delivery\",\n",
    "        \"SAT_modified_delivery\"\n",
    "    ]\n",
    "    , desc='Generating table 1 for categorical variables for each flags'\n",
    "):\n",
    "    ids_to_use = final_df[final_df[x]==1].hosp_id_day_key.unique()\n",
    "    # Groupby aggregation by hospitalization_id\n",
    "    t1_summary = t1_cohort[t1_cohort['hosp_id_day_key'].isin(ids_to_use)].groupby(\"hosp_id_day_key\").agg(\n",
    "        {\n",
    "            \"age_at_admission\": \"mean\",\n",
    "            **{col: documented for col in medication_columns},\n",
    "            **{col: \"first\" for col in demographic_columns},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Apply age bucketing\n",
    "    t1_summary[\"age_bucket\"] = t1_summary[\"age_at_admission\"].apply(age_bucket)\n",
    "\n",
    "    # Drop raw age if you don't need it\n",
    "    t1_summary = t1_summary.drop(columns=[\"age_at_admission\"])\n",
    "\n",
    "    # Reset index if needed\n",
    "    t1_summary = t1_summary.reset_index()\n",
    "\n",
    "    summary_df = t1code.manual_categorical_tableone(\n",
    "        t1_summary, \n",
    "        medication_columns + demographic_columns + [\"age_bucket\"]\n",
    "    )\n",
    "    summary_df.to_csv(f\"{directory_path}/table1_{x}_categorical.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tqdm(\n",
    "    [\n",
    "        \"eligible_event\",\n",
    "        \"SAT_EHR_delivery\",\n",
    "        \"SAT_modified_delivery\",\n",
    "         \"SAT_rass_nonneg_30\",\n",
    "    \"SAT_med_halved_rass_pos\",\n",
    "    \"SAT_no_meds_rass_pos_45\",\n",
    "    'SAT_rass_first_neg_30_last45_nonneg'\n",
    "    ]\n",
    ", desc='Generating Table 1 for continuous variables each flag'):\n",
    "    # --- filter to only the days in this subcohort\n",
    "    ids = final_df.loc[final_df[x] == 1, \"hosp_id_day_key\"].unique()\n",
    "    sub = t1_cohort[t1_cohort[\"hosp_id_day_key\"].isin(ids)]\n",
    "\n",
    "    # --- 1) Day-level medians + flags + demographics\n",
    "    day_summary = (\n",
    "        sub.groupby(\"hosp_id_day_key\")\n",
    "        .agg({**{c: \"median\" for c in continuous_cols}})\n",
    "        .reset_index()\n",
    "    )\n",
    "    summary_df = t1code.manual_tableone(day_summary, continuous_cols)\n",
    "    summary_df.to_csv(f\"{directory_path}/table1_{x}_continuous.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sofa T1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pySofa as sofa\n",
    "sofa.pyCLIF2.helper\n",
    "\n",
    "continuous_cols_sofa = [ 'sofa_cv_97', 'sofa_coag',\n",
    "       'sofa_liver', 'sofa_resp_pf', 'sofa_resp_pf_imp', 'sofa_resp',\n",
    "       'sofa_cns', 'sofa_renal', 'sofa_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_ids = pd.read_csv('../output/intermediate/hospitalization_to_block_df.csv')\n",
    "encounter_dict = dict(zip(mapping_ids['hospitalization_id'].astype(str), mapping_ids['encounter_block'].astype(str)))\n",
    "mapping_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_level_sofa = cohort[['hospitalization_id', 'admission_dttm',\n",
    "       'discharge_dttm']].drop_duplicates().rename(columns={'admission_dttm':'start_dttm','discharge_dttm':'stop_dttm'})\n",
    "encounter_level_sofa = pc.convert_datetime_columns_to_site_tz(encounter_level_sofa, pc.helper['your_site_timezone'])\n",
    "encounter_level_sofa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sout = sofa.compute_sofa(\n",
    "    encounter_level_sofa,\n",
    "    tables_path=None,\n",
    "    use_hospitalization_id = False,\n",
    "    id_mapping = mapping_ids,\n",
    "    group_by_id = \"encounter_block\"\n",
    ")\n",
    "encounter_level_sofa_t1 = t1code.manual_tableone(sout, continuous_cols_sofa)\n",
    "encounter_level_sofa_t1.to_csv(f'{directory_path}/encounter_level_sofa_t1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tqdm(\n",
    "    [\n",
    "        \"on_vent_and_sedation\",\n",
    "        \"sat_delivery_pass_fail\",\n",
    "        \"SAT_EHR_delivery\",\n",
    "        \"SAT_modified_delivery\",\n",
    "        \"SAT_rass_nonneg_30\",\n",
    "        \"SAT_med_halved_rass_pos\",\n",
    "        \"SAT_no_meds_rass_pos_45\",\n",
    "        \"SAT_rass_first_neg_30_last45_nonneg\",\n",
    "    ],\n",
    "    desc=\"Generating Sofa table 1 for each Flags\",\n",
    "):\n",
    "\n",
    "    day_df = df[df[x] == 1][\n",
    "        [\"hospitalization_id\", \"hosp_id_day_key\", \"event_time\"]\n",
    "    ].drop_duplicates()\n",
    "\n",
    "    if day_df.empty:\n",
    "        continue\n",
    "    # Create start_dttm as current_day at 00:00:00\n",
    "    day_df[\"start_dttm\"] = pd.to_datetime(day_df[\"event_time\"]).dt.normalize()\n",
    "    # Create end_dttm as current_day at 23:59:59\n",
    "    day_df[\"stop_dttm\"] = day_df[\"start_dttm\"] + pd.Timedelta(\n",
    "        hours=23, minutes=59, seconds=59\n",
    "    )\n",
    "    day_df = pc.convert_datetime_columns_to_site_tz(\n",
    "        day_df, pc.helper[\"your_site_timezone\"]\n",
    "    )\n",
    "\n",
    "    day_sofa = sofa.compute_sofa(\n",
    "        day_df,\n",
    "        tables_path=None,\n",
    "        use_hospitalization_id=False,\n",
    "        id_mapping=mapping_ids,\n",
    "        group_by_id=\"hosp_id_day_key\",\n",
    "    )\n",
    "\n",
    "    day_sofa_t1 = t1code.manual_tableone(day_sofa, continuous_cols_sofa)\n",
    "    day_sofa_t1.to_csv(f\"{directory_path}/{x}_sofa_t1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thank You!!! upload to box :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".SBT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
